# Тезаурус по курсу YSDA ML

## Tips & Tricks DL

### Предобученная модель
| Term               | Comment                                                                                      |
|--------------------|----------------------------------------------------------------------------------------------|
| Transfer Learning  | Используем знания из одной задачи, чтобы ускорить обучение на схожей новой задаче и достичь лучшего результата быстрее |
| Fine-tuning        | Настраиваем предобученную модель на новом датасете, изменяем веса последних слоев под специфику задачи |

### Сходимость
| Term         | Comment                                                                                      |
|--------------|----------------------------------------------------------------------------------------------|
| Learning Rate| Определяет шаг, с которым обновляются веса. Слишком большой - обучение нестабильно, слишком маленький - очень медленно |
| Convergence  | Процесс, когда модель приближается к минимуму функции потерь и метрики перестают значительно меняться |

### Переобучение
| Term            | Comment                                                                                   |
|-----------------|-------------------------------------------------------------------------------------------|
| Overfitting     | Модель учится на шуме и выбросах тренировочных данных, плохо работает на новых примерах  |
| Regularization  | Методы (L1/L2, Dropout, Data Augmentation) для предотвращения переобучения и улучшения обобщающей способности |

### Визуализация
| Term                     | Comment                                                                                      |
|--------------------------|----------------------------------------------------------------------------------------------|
| Feature Map Visualization| Показываем активации свёрточных слоёв, чтобы понять, какие признаки выучила сетка и как она их обрабатывает |

### Свёртки и пуллинги
| Term          | Comment                                                                                          |
|---------------|--------------------------------------------------------------------------------------------------|
| Convolution   | ядро поэлементно умножается на участок входа, выявляя локальные паттерны     |
| Kernel        | Обучаемая матрица внутри свёрточного слоя, отвечающая за распознавание определённых признаков     |
| Padding       | Дополняем границы нулями (или дублируем пиксели), чтобы сохранить размер изображения после свёртки |
| Stride        | Расстояние между позициями ядра. Увеличивает или уменьшает детализацию выходного признака        |
| Dilation      | Расширение ядра за счёт "пропусков" между элементами, увеличивает receptive field без роста числа параметров |
| MaxPooling    | Выбираем максимальное значение в каждом окне - сокращаем размер и фокусируемся на сильнейших признаках |
| AvgPooling    | Усредняем значения в окне - сглаживаем шум и сокращаем пространство признаков                   |
| Receptive Field | Область исходного изображения, на которую "смотрит" один нейрон, чем глубже слой, тем она больше |
| Backpropagation | Алгоритм обратного распространения ошибки для вычисления градиентов и обновления весов сетки    |

## Attention & Transformer

### Эмбеддинги
| Term              | Comment                                                                                                                 |
|-------------------|-------------------------------------------------------------------------------------------------------------------------|
| One-hot Encoding  | Представление слов в виде векторов с единицей на позиции слова и нулями в остальных                                     |
| Bag-of-Words      | Сумма one-hot векторов. Учитывает только частотность, игнорирует порядок слов                                           |
| Word2Vec          | Векторное представление основывается на контекстной близости - слова, встречающиеся в тексте рядом с одинаковыми словами, будут иметь близкие векторы                   |
| GloVe             | Глобальные векторные представления слов на основе матрицы совместных встречаемостей                                     |
| FastText          | Позволяет строить векторы для редких или новых слов через субсловные представления                  |

### Внимание
| Term           | Comment                                                                                             |
|----------------|-----------------------------------------------------------------------------------------------------|
| Attention      | Механизм, который взвешивает вклад каждого элемента входа при формировании представления             |
| Self-Attention | Каждый токен взаимодействует со всеми другими, вычисляя вес (важность) каждого при построении эмбеддинга |

### Архитектура Transformer
| Term                 | Comment                                                                                        |
|----------------------|------------------------------------------------------------------------------------------------|
| Multi-Head Attention | Параллельно применяем несколько голов внимания, чтобы улавливать разные аспекты зависимости     |
| Positional Encoding  | Добавляем информацию о порядке токенов через синусоиды или обучаемые векторы                   |
| Residual Connection  | Пропускаем исходный сигнал и складываем с выходом слоя, чтобы облегчить обучение глубоких сеток |
| Layer Normalization  | Стабилизируем обучение, нормализуя активации по каналам внутри одного сэмпла                    |

### RNN и производные
| Term | Comment                                                                                   |
|------|-------------------------------------------------------------------------------------------|
| RNN  | Рекуррентная сетка для обработки последовательностей, но страдает затухающим/взрывающимся градиентом |
| LSTM | Добавляет механизмы гейтов - забывания, входа и выхода, для работы с долгосрочными зависимостями    |
| GRU  | Упрощённая версия LSTM с единым гейтом обновления и забывания, быстрее в обучении                 |

### Задачи NLP
| Term | Comment                                                                                               |
|------|-------------------------------------------------------------------------------------------------------|
| MLM  | Masked Language Modelling - предсказываем скрытые токены, учимся контекстному представлению текста    |
| NSP  | Next Sentence Prediction - модель решает, идёт ли второе предложение за первым в оригинальном тексте   |

### Адаптация больших моделей
| Term | Comment                                                                                                      |
|------|--------------------------------------------------------------------------------------------------------------|
| PEFT | Parameter-Efficient Fine-Tuning — методы (LoRA, Prompt Tuning) для экономного дообучения больших моделей     |
| LoRA | Low-Rank Adaptation - вставляем низкоранговые модули, не меняя основную матрицу весов                        |
| Prompt Tuning | Обучаем дополнительные токены (промпты), чтобы адаптировать модель без изменения основных весов |

## Интерпретация нейросетей

### Основные понятия
| Term              | Comment                                                                                                |
|-------------------|--------------------------------------------------------------------------------------------------------|
| Интерпретируемость| Насколько человек может понять причины и логику решений модели                                          |
| Объяснимость      | Способность дать понятное, компактное объяснение работы алгоритма или конкретного предсказания         |

### Model-agnostic
| Term               | Comment                                                                                          |
|--------------------|--------------------------------------------------------------------------------------------------|
| LIME               | Локально обучаемая интерпретируемая модель: строим простую аппроксимацию вокруг одного примера   |
| SHAP               | Распределяем вклад каждого признака через теорему Шепли, получаем согласованные меры важности    |
| Feature Importance | Оцениваем влияние каждого признака на итоговое качество предсказаний модели                     |

### Визуализация
| Term                   | Comment                                                                                  |
|------------------------|------------------------------------------------------------------------------------------|
| Saliency Map           | Визуализация градиентов выходного сигнала по входным пикселям, показывает, на что "смотрит" сетка |
| Partial Dependence Plot| График зависимости среднего предсказания от изменения одного признака при фиксированных других |

### Контрфактуальные объяснения
| Term                        | Comment                                                                                             |
|-----------------------------|-----------------------------------------------------------------------------------------------------|
| Counterfactual Explanations | Ищем минимальные изменения во входе, которые приведут модель к другому выводу - объясняем границы решения |

## Generative Models

### Основы (Part 1)
| Term                | Comment                                                                                                              |
|---------------------|----------------------------------------------------------------------------------------------------------------------|
| Generative Model    | Модель, способная генерировать новые образцы, приближая истинное распределение данных                                |
| Distribution Matching| Поиск параметров, минимизируя меру расстояния между эмпирическим и модельным распределениями                         |

### Сравнение распределений
| Term                   | Comment                                                                                                               |
|------------------------|-----------------------------------------------------------------------------------------------------------------------|
| KL Divergence          | Несимметричная мера расхождения двух распределений на основе логотношения их плотностей                             |
| JS Divergence          | Симметричная версия на базе KL к смеси распределений. Всегда конечна и ограничена                                     |
| Total Variation Distance| Прямое интегральное расстояние между плотностями для оценки максимальной разницы                                     |
| Wasserstein Distance   | "Earth Movers" расстояние - минимальная работа по преобразованию одного распределения в другое                      |

### Автоэнкодеры
| Term                         | Comment                                                                                       |
|------------------------------|-----------------------------------------------------------------------------------------------|
| Autoencoder                  | сжимаем информацию в латентное пространство и восстанавливаем обратно |
| Variational Autoencoder      | Добавляем вероятностную компоненту и регуляризацию латента через KL Divergence к нормальному |
| Reparameterization Trick     | выносим случайность за скобки           |

### GAN
| Term             | Comment                                                                                     |
|------------------|---------------------------------------------------------------------------------------------|
| GAN              | Генератор пытается обмануть дискриминатор, а дискриминатор учится распознавать подделки |
| Generator        | Сетка, создающая новые примеры из случайного шума или латентных векторов                     |
| Discriminator    | Классификатор, отличающий реальные данные от сгенерированных, стимулирует генератор к улучшению |
| Adversarial Loss | Мин-макс лосс, синхронно обучающий две сетки к оптимальному "балансу сил"                     |
| Mode Collapse    | Проблема, когда генератор выдаёт однотипные примеры и игнорирует часть распределения          |

### Normalizing Flows (Part 2)
| Term            | Comment                                                                                                              |
|-----------------|----------------------------------------------------------------------------------------------------------------------|
| Normalizing Flow| Обратимая цепочка дифференцируемых преобразований с вычисляемым якобианом для плотностей                             |
| Planar Flow     | Простая архитектура потока с одним скрытым слоем и аналитическим вычислением детерминанта якобиана                   |
| Radial Flow     | Расширяем плотность вокруг центра, сохраняя способность расчёта изменения объёма через якобиан                       |

### Diffusion Models
| Term                        | Comment                                                                                                  |
|-----------------------------|----------------------------------------------------------------------------------------------------------|
| Diffusion Model             | Учимся постепенно шумить данные и затем применять обратный процесс денойзинга для генерации               |
| Denoising Score Matching    | Обучаем оценку градиента плотности для каждого уровня шума, чтобы восстанавливать образец                |
| Classifier Guidance         | Используем классификатор для корректировки процесса генерации в условных диффузионных моделях            |

## Reinforcement Learning

### Основные понятия
| Term                   | Comment                                                                                                   |
|------------------------|-----------------------------------------------------------------------------------------------------------|
| Markov Decision Process| Формализует RL через состояния, действия, награды и вероятности переходов                                 |
| Policy                 | Правило выбора действий в зависимости от текущего состояния                              |
| Value Function         | Оценивает ожидаемую кумулятивную награду, начиная из данного состояния                                    |
| Q-Function             | Оценивает ценность конкретного действия в заданном состоянии                                               |
| Bellman Equation       | Рекурсивное соотношение для V и Q, фундамент алгоритмов динамического программирования                      |
| Reward Discount        | Коэффициент, уменьшающий важность дальних наград при расчёте общей суммы                                   |

### Алгоритмы и приёмы
| Term               | Comment                                                                                                          |
|--------------------|------------------------------------------------------------------------------------------------------------------|
| Epsilon-Greedy     | Балансируем исследование и использование, с вероятностью ε случайно выбираем действие                            |
| Behavior Cloning   | Клонируем поведение эксперта через обычное обучение с учителем, но рискуем смещением распределения               |
| DAGGER             | Итеративно собираем данные эксперта на поведении агента, уменьшаем distributional shift                         |
| Cross-Entropy Method | выбираем лучшие параметры политики, обновляем распределение над ними                    |
| Adversarial Attack | Создаём "врагов" для модели, добавляем шум или патчи, чтобы обмануть предсказание                               |

## Временные ряды

### Ключевые концепции
| Term              | Comment                                                                                              |
|-------------------|------------------------------------------------------------------------------------------------------|
| Stationarity      | Свойство ряда, при котором статистики (среднее, дисперсия) не зависят от времени                      |
| Autocorrelation   | Корреляция значений временного ряда с его лагающими копиями                                          |
| Seasonality       | Периодические колебания с фиксированным интервалом (день, неделя, год)                                |
| Trend             | Долгосрочное направление изменения ряда вверх или вниз                                                |
| Differencing      | Вычитание предыдущего значения для устранения тренда и достижения stationarity                       |

### Модели
| Term               | Comment                                                                                            |
|--------------------|----------------------------------------------------------------------------------------------------|
| ARIMA              | Авторегрессия + интегрирование + скользящее среднее для гибкого прогнозирования ряда               |
| SARIMA             | Сезонная ARIMA - учитываем как обычные, так и сезонные компоненты ряда                             |
| Exponential Smoothing| Скользящее экспоненциальное сглаживание с разными весами для новых и старых наблюдений           |

### Признаки для ML
| Term          | Comment                                                                                          |
|---------------|--------------------------------------------------------------------------------------------------|
| Lag Features  | Используем прошлые значения ряда как признаки для модели машинного обучения                      |
| Rolling Window| Считаем статистики (среднее, std, min/max) по окну предыдущих наблюдений (3 дня/7 дней и т.п)    |

## Индустриальные и исследовательские аспекты

### ML Engineering vs ML Research
| Term           | Comment                                                                                       |
|----------------|-----------------------------------------------------------------------------------------------|
| ML Engineering | Применение ML в бизнесе - оптимизация ключевых метрик, работа с "личными" данными      |
| ML Research    | публикация статей, создание новых методов, рокетсайнс                     |

### Research Areas (YSDA)
| Term            | Comment                                                                    |
|-----------------|----------------------------------------------------------------------------|
| Tabular ML      | Как адаптировать сетки для табличных данных и "обойти" классические модели |
| Graph ML        | Методы работы с графами - агрегация соседей, предсказание узлов и ребёр     |
| Efficient DL    | Оптимизация моделей (квантизация, отсечение, дистилляция) для быстрого инференса |
| Generative DL   | Создание мультимодальных и условных генеративных моделей высокого качества |

## Общие термины и инструменты

### Common Operations
| Term             | Comment                                                                                |
|------------------|----------------------------------------------------------------------------------------|
| ReduceLROnPlateau| Понижаем LR, если лосс не улучшается в течение нескольких эпох                |
| StepLR           | Уменьшаем LR через фиксированное число шагов обучения                                   |
| SequentialLR     | Последовательное применение нескольких LR шедулеров для тонкого контроля            |

### Common Tasks
| Term              | Comment                                                                                             |
|-------------------|-----------------------------------------------------------------------------------------------------|
| Distillation      | Учим компактную модель по "мягким" ответам большой pretrained модели                               |
| Quantization      | Приведение весов и активаций к низкой битности для ускорения инференса                             |
| Pretraining       | Обучение модели на большой общей задаче перед специализированным дообучением                       |
| Metric Learning   | Учим функцию расстояния для сравнения объектов (face verification)                                    |

### Common Terms in Architectures
| Term              | Comment                                                                                     |
|-------------------|---------------------------------------------------------------------------------------------|
| Backbone          | Базовая часть сетки для извлечения признаков, повторно используемая в разных задачах        |
| Head              | Конечная часть сетки, отвечающая за конкретное предсказание (классификация, регрессия)       |
| Gating Mechanism  | Компонент с «вратами» для управления потоком информации внутри сетки                         |
| Gradient Clipping | Ограничиваем норму градиентов, чтобы избежать их взрыва при обучении                        |

### Probability & Processes
| Term            | Comment                                                                                             |
|-----------------|-----------------------------------------------------------------------------------------------------|
| Markov Process  | Процесс, в котором будущее состояние зависит только от текущего, а не от всей предыстории            |

### Интерпретируемость и регуляризация
| Term         | Comment                                                                                           |
|--------------|---------------------------------------------------------------------------------------------------|
| Data Noising | Добавляем шум к входным данным, чтобы повысить устойчивость и обобщающую способность модели      |

### Libraries & Tools
| Term              | Comment                                                                                           |
|-------------------|---------------------------------------------------------------------------------------------------|
| PyTorch           | Фреймворк для разработки сеток и быстрого прототипирования                           |
| albumentations    | Библиотека аугментаций изображений с простым интерфейсом                                  |
| einops            | Интуитивные операции по перестановке и переупорядочению тензоров                                  |
| Hugging Face      | Сисчтема для работы с трансформерами - модели, датасеты, токенизаторы, diffusers                 |
| Optuna            | Инструмент для автоматического поиска гиперпараметров (круче gridsearch, быстрее и т.п.)              |
| Streamlit         | Лёгкий способ превращать ML-скрипты в веб-приложения (удобно для PoC/MVP)                                |