# Тезаурус по курсу YSDA ML

## Tips & Tricks DL

### Предобученная модель
| Термин             | Определение                                                                                    | Пример/Комментарий                                                                                                           |
|--------------------|------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------|
| Transfer Learning  | Используем знания из одной задачи, чтобы ускорить обучение на схожей новой задаче и достичь лучшего результата быстрее         | Сохранили базовые слои и доучили под новую задачу. |
| Fine-tuning        | Настраиваем предобученную модель на новом датасете, изменяем веса последних слоев под специфику задачи           | Дообучаем последние слои BERT на корпусе вопросов поддержки клиентов, чтобы улучшить ответы.      |

### Сходимость
| Термин         | Определение                                                                                                                   | Пример/Комментарий                                                                                |
|----------------|----------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| Learning Rate  | Определяет шаг, с которым обновляются веса. Слишком большой - обучение нестабильно, слишком маленький - очень медленно    | Если при lr=0.1 модель "скачет" вокруг минимума, понижаем до 0.01 и получаем более плавное уменьшение лосса.  |
| Convergence    | Процесс, когда модель приближается к минимуму функции потерь и метрики перестают значительно меняться | После 20й эпохи лосс перестал падать сильнее 0.001 — можно считать, что модель сошлась.                                     |

### Переобучение
| Термин            | Определение                                                                                                 | Пример/Комментарий                                                                                                           |
|-------------------|-------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| Overfitting       | Модель учится на шуме и выбросах тренировочных данных, плохо работает на новых примерах                     | Модель на тренировочных данных даёт 99% точности, но на тесте падает до 65% - признак переобучения.                       |
| Regularization    | 	Методы (L1/L2, Dropout, Data Augmentation) для предотвращения переобучения и улучшения обобщающей способности     | Добавили Dropout p=0.5 в полносвязные слои, это снизило переобучение и подняло accuracy на валидации на 5%.               |

### Визуализация
| Термин                     | Определение                                                                                                 | Пример/Комментарий                                                                                                           |
|-------------------|-------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| Feature Map Visualization  | Показываем активации свёрточных слоёв, чтобы понять, какие признаки выучила сетка и как она их обрабатывает          | Визуализировали первые 16 feature maps - в первом слое сетка реагирует на края и текстуры, а в глубоком - на сложные формы.  |

### Свёртки и пуллинги
| Термин            | Определение                                                                                                 | Пример/Комментарий                                                                                                           |
|-------------------|-------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| Convolution       | ядро поэлементно умножается на участок входа, выявляя локальные паттерны                                    | 3x3 kernel по RGB-изображению выявляет вертикальные границы объектов.                                                |
| Kernel            | Обучаемая матрица внутри свёрточного слоя, отвечающая за распознавание определённых признаков               | 5x5 kernel может обучиться различать круги на изображении, отвечая сильнее на круглые контуры.                             |
| Padding           | Дополняем границы нулями (или дублируем пиксели), чтобы сохранить размер изображения после свёртки          | добавили по одному нулю вокруг, выходное изображение осталось 28x28 вместо 26x26.                            |
| Stride            | Расстояние между позициями ядра. Увеличивает или уменьшает детализацию выходного признака                    | Stride=2 на 32x32 изображении даст выход 16x16 - полезно для быстрой свёртки.                                                |
| Dilation          | Расширение ядра за счёт "пропусков" между элементами, увеличивает receptive field без роста числа параметров   | Dilation=2 на 3x3 kernel ведёт себя как 5x5, но с 9 параметрами, захватывая более крупные паттерны.                        |
| MaxPooling        | Выбираем максимальное значение в каждом окне - сокращаем размер и фокусируемся на сильнейших признаках | MaxPool2D(2x2) на feature map 16×16 даст 8×8, сохраняя наиболее выраженные активации.                                       |
| AvgPooling        | Усредняем значения в окне - сглаживаем шум и сокращаем пространство признаков                                | AvgPool2D(2x2) на яркостном канале снимка смягчает резкие перепады и уменьшает шум.                                          |
| Receptive Field   | Область исходного изображения, на которую "смотрит" один нейрон, чем глубже слой, тем она больше             | В третьем слое свёртка с padding и stride 1 даёт RF=7×7 - нейрон "видит" паттерны размером до 7 пикселей.                    |
| Backpropagation   | Алгоритм обратного распространения ошибки для вычисления градиентов и обновления весов сетки                  | При обучении нейронки для распознавания цифр она вычисляет разницу между предсказанной и истинной метками, а затем распространяет этот градиент назад через слои, корректируя веса для уменьшения ошибки..                            |

## Attention & Transformer

### Эмбеддинги
| Термин               | Определение                                                                                                  | Пример/Комментарий                                                                                                       |
|-------------------|-------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| One-hot Encoding     | Представление слов в виде векторов с единицей на позиции слова и нулями в остальных                       | Слово "cat" в словаре из 5 слов может быть [0,1,0,0,0], где 1 на позиции "cat".                                             |
| Bag-of-Words         | Сумма one-hot векторов. Учитывает только частотность, игнорирует порядок слов                                                  | Для текста «кот кот мышь» BoW будет [2,1,0,0,…], где число - счётчик каждого слова.                    |
| Word2Vec             | Векторное представление основывается на контекстной близости - слова, встречающиеся в тексте рядом с одинаковыми словами, будут иметь близкие векторы  | Вектор "король" - "мужчина" + "женщина" ~ "королева" показывает семантическую пригодность.    |
| GloVe                | Глобальные векторные представления слов на основе матрицы совместных встречаемостей                              | GloVe пытается найти векторы слов так, чтобы их скалярное произведение было близко к логарифму числа совместных встречаемостей.       |
| FastText             | Позволяет строить векторы для редких или новых слов через субсловные представления                                     | Слово playing разбивается на 3-граммы "pl", "pla", "lay", "ayi", "yin", "ing", и вектор слова - сумма их эмбеддингов.     |

### Внимание
| Термин           | Определение                                                                                                  | Пример/Комментарий                                                                                                           |
|------------------|--------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| Attention        | Механизм вычисления весов важности элементов входной последовательности                                      | В задаче перевода "она ест" из всех слов "она", "ест", "end" выбирает наиболее релевантные при генерации следующего токена. |
| Self-Attention   | Каждый токен взаимодействует со всеми другими, вычисляя вес (важность) каждого при построении эмбеддинга         | При кодировании предложения "The cat sat on the mat" каждый токен смотрит на все остальные и формирует контекстный вектор.   |

### Архитектура Transformer
| Термин                 | Определение                                                                                                 | Пример/Комментарий                                                                                                           |
|------------------------|-------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| Multi-Head Attention   | Параллельное применение нескольких attention-голов для улавливания разных типов зависимостей                  | Одна голова улавливает синтаксис, другая — семантику: в совокупности даёт более богатое контекстное представление.            |
| Positional Encoding    | Добавляем информацию о порядке токенов через синусоиды или обучаемые векторы                   | Для i-го токена и-й фичей вычисляем sin(i/10000^(2j/d)), что позволяет сети учитывать порядок слов даже без RNN.             |
| Residual Connection    | Пропускаем исходный сигнал и складываем с выходом слоя, чтобы облегчить обучение глубоких сеток          | После Multi-Head Attention складываем его выход с исходным входом (и нормализуем), чтобы ускорить сходимость глубоких сетей.|
| Layer Normalization    | Стабилизируем обучение, нормализуя активации по каналам внутри одного сэмпла                   | Применяем нормализацию по каналам после add&norm: стабилизируем распределение активаций и ускоряем обучение.                |

### RNN и производные
| Термин | Определение                                                                                                 | Пример/Комментарий                                                                                                           |
|--------|-------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| RNN    | Рекуррентная сетка для обработки последовательностей, где состояние зависит от предыдущего шага               | при длинных предложениях градиенты затухают, сложно моделировать долгосрочные зависимости.|
| LSTM   | Добавляет гейты (забывания, входа, выхода) для работы с долгосрочными зависимостями                         | Подходят для задач перевода и речевого распознавания — запоминают информацию на десятках шагов текста без "забывания".      |
| GRU    | Упрощённая версия LSTM с единым гейтом обновления и забывания, быстрее обучается                             | Часто показывает близкое качество к LSTM, но обучается быстрее и с меньшим числом параметров.                               |

### Задачи NLP
| Термин | Определение                                                                                                 | Пример/Комментарий                                                                                                           |
|--------|-------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| MLM    | Masked Language Modelling — предсказывание скрытых токенов по контексту                                      | В предложении «Я люблю ___ кофе» модель угадывает «пить», учась понимать контекст.                                         |
| NSP    | Next Sentence Prediction — модель решает, идёт ли второе предложение за первым в оригинальном текст         | BERT обучался на задачах, где нужно решать, идёт ли второе предложение за первым в оригинале.                               |

### Адаптация больших моделей
| Термин         | Определение                                                                                                 | Пример/Комментарий                                                                                                           |
|----------------|-------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------|
| PEFT           | Parameter-Efficient Fine-Tuning — методы (LoRA, Prompt Tuning) для экономного дообучения больших моделей       | LoRA: добавляем маленькие low-rank матрицы к весам, не трогая оригинальные параметры, что экономит память и время обучения.  |
| LoRA           | Low-Rank Adaptation — вставляем низкоранговые модули, не меняя основную матрицу весов                             | При дообучении LLM обновляем только параметры LoRA-модулей, а основной вес остаётся замороженным.                           |
| Prompt Tuning  | Обучаем дополнительные токены (промпты), чтобы адаптировать модель без изменения основных весов                       | Вместо fine-tuning добавляем 20 токенов в начало каждого запроса и учим их так, чтобы получить нужные ответы от LLM.         |

---

## Интерпретация нейросетей

### Основные понятия
| Термин               | Определение                                                                                  | Пример/Комментарий                                                                                          |
|----------------------|----------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| Интерпретируемость   | Насколько человек может понять причину и логику решения модели                               | Если можно объяснить, почему сетка классифицировала опухоль как злокачественную — повышается доверие врачу.   |
| Объяснимость         | Способность дать понятное, компактное объяснение работы алгоритма или конкретного предсказания | Например, «модель оценило кредитоспособность низкой из-за высокого коэффициента задолженности».             |

### Model-agnostic методы
| Термин               | Определение                                                                                  | Пример/Комментарий                                                                                          |
|----------------------|----------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| LIME                 | Строит простую линейную модель вокруг конкретного примера, чтобы объяснить его предсказание | Обучили логистическую регрессию на искаженном классификатором фоне одного изображения, чтобы понять, что важно — цвет или форма. |
| SHAP                 | Распределяем вклад каждого признака через теорему Шепли, получаем согласованные меры важности       | Для рейтинга недвижимости SHAP показал, что площадь и район дали 80 % вклада в прогноз цены.               |
| Feature Importance   | Оцениваем влияние каждого признака на итоговое качество предсказаний модели                                 | В RandomForest важно число комнат — этот признак даёт наибольшее снижение лосса при его перестановке.         |

### Визуализация
| Термин                   | Определение                                                                        | Пример/Комментарий                                                                                          |
|--------------------------|------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| Saliency Map             | Градиенты выходного класса по входным пикселям                                       | На кошачьих фото карта важности подсветила контуры ушей и усов — именно они решают классификацию «кот».     |
| Partial Dependence Plot  | График среднего предсказания модели при изменении одного признака                   | Для дерева решений показали, как меняется риск отказа кредита при росте суммы займа от 100 к до 1 млн.     |

### Контрфактуальные объяснения
| Термин                        | Определение                                                                         | Пример/Комментарий                                                                                          |
|-------------------------------|-------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| Counterfactual Explanations   | Ищем минимальные изменения во входе, которые приведут модель к другому выводу      | Вам отказали в выдаче кредита. Если бы ваш ежемесячный доход был на 15 000 ₽ выше, заявку бы одобрили    |

---

## Generative Models

### Основы (Part 1)
| Термин               | Определение                                                                                   | Пример/Комментарий                                                                                          |
|----------------------|-----------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| Generative Model     | Модель, способная генерировать новые данные, приближая истинное распределение данных          | VAE/GAN/Diffusion могут создавать фотореалистичные лица, не существующие в датасете.                    |
| Distribution Matching| Поиск параметров, минимизируя меру расстояния между эмпирическим и модельным распределениями           | При обучении VAE минимизируем KL Divergence между латентным распределением и нормальным.              |

### Сравнение распределений
| Термин                    | Определение                                                                                  | Пример/Комментарий                                                                                              |
|---------------------------|----------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|
| KL Divergence             | Несимметричная мера расхождения двух распределений           | KL-дивергенция между истинным распределением P=(0.7, 0.3) и модельным Q=(0.5, 0.5) (~0.08) показывает, сколько дополнительной информации нужно, чтобы закодировать данные из P, используя Q. |
| JS Divergence             | Симметричная дивергенция на основе KL к смеси распределений                                   | JS-дивергенция между распределениями P=(0.9, 0.1) и Q=(0.5, 0.5) примерно 0.1 и даёт симметричную меру их различия.              |
| Total Variation Distance  | Интегральная мера максимальной разницы между плотностями                                     | Используется редко из-за жёсткости, но показывает максимальное отличие реального и сгенерированного распределений.|
| Wasserstein Distance      | "Earth Movers" расстояние - минимальная работа по преобразованию одного распределения в другое | WGAN применяет Wasserstein для стабильного обучения генератора без проблем mode collapse.                    |

### Автоэнкодеры
| Термин                    | Определение                                                                                  | Пример/Комментарий                                                                                            |
|---------------------------|----------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|
| Autoencoder               | сжимаем информацию в латентное пространство и восстанавливаем обратно            | Кодируем изображение в вектор размером 32 и декодируем обратно, сравниваем с оригиналом по MSE.                |
| Variational Autoencoder   | Добавляем вероятностную компоненту и регуляризацию латента через KL Divergence к нормальному    | В VAE вместо точки в латенте получаем распределение N(μ,σ), что позволяет плавно генерировать новые точки.   |
| Reparameterization Trick  | выносим случайность за скобки                                                            | Вместо z~N(μ,σ²) пишем z=μ+σ·ε, ε~N(0,1), чтобы градиенты шли через μ и σ.                                     |

### GAN
| Термин             | Определение                                                                                 | Пример/Комментарий                                                                                          |
|--------------------|---------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| GAN                | Генератор пытается обмануть дискриминатор, а дискриминатор учится распознавать подделки         | Генератор пытается обмануть дискриминатор фейковыми изображениями, дискриминатор — распознать фейки.         |
| Generator          | сетка, создающая новые данные из шума или латентных векторов                                 | В генераторе DCGAN добавили ConvTranspose слои, чтобы развёртывать векторы 100->64x64 изображение.           |
| Discriminator      | сетка, отличающая реальные данные от сгенерированных                                          | CNN-дискриминатор находит паттерны артефактов в сгенерированных изображениях и сообщает генератору об ошибках.|
| Adversarial Loss   | Мин‐макс лосс, обучающий две сети к оптимальному балансу                                      | генератор минимизирует, дискриминатор максимизирует.                       |
| Mode Collapse      | Проблема, когда генератор выдаёт однотипные данные и игнорирует часть распределения         | Генератор научился выдавать только "улыбчивые лица" и перестал генерировать другие эмоции.                   |

### Normalizing Flows (Part 2)
| Термин            | Определение                                                                                  | Пример/Комментарий                                                                                          |
|-------------------|----------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| Normalizing Flow  | Обратимая цепочка дифференцируемых преобразований с вычисляемым якобианом для плотностей | берёт простое нормальное распределение и с помощью серии обратимых преобразований формирует сложное распределение данных, при этом точно вычисляя его плотность.   |
| Planar Flow       | Простая архитектура потока с одним скрытым слоем и аналитическим вычислением детерминанта якобиана                | Добавляем к x небольшое нелинейное и обратимое смещение, легко считаем детерминант якобиана.                |
| Radial Flow       | Расширяем плотность вокруг центра, сохраняя способность расчёта изменения объёма через якобиан                   | радиальный флоу применяет обратимое смещение радиально сжимая или расширяя распределение вокруг точки z0  |

### Diffusion Models
| Термин                    | Определение                                                                               | Пример/Комментарий                                                                                          |
|---------------------------|-------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| Diffusion Model           | Последовательный стохастический шум и обратный процесс денойзинга для генерации данных    | DDPM шумит изображение в 1000 шагов, затем учится «шаг за шагом» восстанавливать исходное.                   |
| Denoising Score Matching  | Обучение градиента лог-плотности для каждого уровня шума, чтобы правильно денойзить       | Модель учится предсказывать ∇ₓ log pₜ(xₜ) для каждого t, что позволяет точно «откачивать» шум.             |

---

## Reinforcement Learning

### Основные понятия
| Термин                   | Определение                                                                                 | Пример/Комментарий                                                                                          |
|--------------------------|---------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| Markov Decision Process  | состояние, действие, награда, вероятность перехода                               | В задаче игры «змейка» s=положение змеи, a=направление движения, r=+1 за съеденную еду, дальше случайности нет. |
| Policy                   | правило или стратегия, по которому агент выбирает действие в каждом состоянии.                        |  например, в игре "лабиринт" политика может быть "если впереди стена – поверни направо".     |
| Value Function           | оценка того, сколько вознаграждения агент ожидает получить в будущем, начав из данного состояния.          | если V(клетка A)=0.8, значит, в среднем из A можно заработать 0.8 единиц награды.               |
| Q-Function               | оценка того, сколько вознаграждения агент ожидает получить, если из состояния s выполнит действие a и далее будет следовать своей политике.          | Q(состояние=S, действие=двигаться влево)=0.5 означает, что при таком действии ожидается 0.5 единиц награды.              |
| Bellman Equation         |  рекурсивная формула, которая выражает ценность состояния или пары (s,a) через вознаграждение и ценности следующих состояний.    | V(s)=r(s,a)+γ·V(s′) показывает, что ценность s равна полученному вознаграждению r плюс дисконтированная ценность следующего состояния s′.    |
| Reward Discount (γ)      |  коэффициент γ∈[0,1], уменьшающий вес будущих вознаграждений, чтобы предпочесть ближние по времени.        | при γ=0.9 награда через два шага учитывается как 0.9² ~ 0.81 текущей.                    |

### Алгоритмы и приёмы
| Термин               | Определение                                                                                 | Пример/Комментарий                                                                                          |
|----------------------|---------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| Epsilon-Greedy       | стратегия выбора действий, при которой с вероятностью ε агент случайно исследует среду, а с вероятностью 1−ε — выбирает действие с наивысшей оценкой           | при ε=0.1 в 10 % эпизодов агент делает случайный ход, а в остальные 90 % опирается на текущие Q-оценки.       |
| Behavior Cloning     |  имитационное обучение, где агент обучается копировать поведение эксперта, используя собранные пары "состояние -> действие".         | агент-автопилот учится рулить по видеозаписям настоящего водителя, повторяя его манёвры.     |
| DAGGER               | итеративный алгоритм имитационного обучения, где агент генерирует собственные траектории, эксперт помечает их "правильными" действиями, и эти данные добавляются в обучающий набор.        | агент играет в го, сам делает ходы, эксперт подменяет их оптимальными, и новая информация дообучает модель.    |
| Cross-Entropy Method | стохастический метод оптимизации, где на каждом шаге генерируется выборка кандидатов, выбираются лучшие по вознаграждению и на их основе обновляется распределение параметров, минимизируя кросс-энтропию.   | для настройки робота-манипулятора генерируют тысячи конфигураций, отбирают топ-5 % по точности захвата и с их помощью корректируют параметры.      |
| Adversarial Attack   | целенаправленное малозаметное искажение входных данных, которое заставляет модель совершать ошибку.                   | к изображению "панды" добавляют почти незаметный шум, и классификатор начинает видеть "гриб".  |

---

## Временные ряды

### Ключевые концепции
| Термин              | Определение                                                                              | Пример/Комментарий                                                                                          |
|---------------------|------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| Stationarity       | Свойство, при котором статистики ряда (среднее, дисперсия) не меняются со временем         | После дифференцирования цен на акции ряд становится stationary — можно применять ARMA без дополнительных приёмов. |
| Autocorrelation     | Корреляция ряда с его лагающими копиями                                                  | ACF показывает, что продажи мороженого летом коррелируют с прошлым годом на лаге 12 месяцев.                  |
| Seasonality        | Периодические колебания с фиксированным периодом                                         | Ежемесячные продажи пива растут каждый июль — сильная годовая сезонность.                                     |
| Trend              | Долгосрочная направленность ряда вверх или вниз                                          | Рост интернет-торговли показывает постоянный восходящий тренд в продажах.                                     |
| Differencing       | Вычитание предыдущего значения для устранения тренда и достижения stationarity           | Применяем d=1: yₜ′=yₜ−yₜ₋₁, это убирает линейный тренд из временного ряда.                                  |

### Модели
| Термин                 | Определение                                                                           | Пример/Комментарий                                                                                          |
|------------------------|---------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| ARIMA                  | Авторегрессия (AR) + интеграция (I) + скользящая средняя (MA)                         | ARIMA(2,1,1) на продажах электроэнергии учитывает два AR-лага, одну разность и одну MA-компоненту.        |
| SARIMA                 | Сезонная ARIMA. Учитывает и обычные, и сезонные компоненты                             | SARIMA(1,1,1)(1,1,1,12) моделирует месячные продажи с годовой сезонностью.                                 |
| Exponential Smoothing  | Экспоненциальное сглаживание с одним или несколькими уровнеми         | Holt–Winters учёл и тренд, и сезонность при прогнозе температуры на следующий год.                         |

### Признаки для ML
| Термин           | Определение                                                                            | Пример/Комментарий                                                                                          |
|------------------|----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| Lag Features     | Использование предыдущих значений ряда как признаков                                   | Для прогноза завтра берём yₜ, yₜ₋₁, yₜ₋₂ как признаки в линейной регрессии.                                 |
| Rolling Window   | Статистики (среднее, std, min/max) по окну из N последних наблюдений                   | Считаем среднюю температуру за последние 7 дней, чтобы предсказать сегодняшнюю температуру.                  |

---

## Индустриальные и исследовательские аспекты

### ML Engineering vs ML Research
| Термин           | Определение                                                                            | Пример/Комментарий                                                                                          |
|------------------|----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| ML Engineering   | Применение ML для решения бизнес-задач и оптимизации ключевых метрик            | Рекомендательная система для e-com, которая подняла конверсию на 3 %.                                    |
| ML Research      | Новые методы, публикации, открытые бенчмарки                      | Разработка нового attention-механизма и публикация статьи на NeurIPS.                                       |

### Research Areas (YSDA)
| Термин           | Определение                                                                            | Пример/Комментарий                                                                                          |
|------------------|----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| Tabular ML       | Адаптация нейросетей и гибридных методов к табличным данным                            | TabM от Yandex Research показывает, что нейросети могут обгонять бустинг на табличных задачах.             |
| Graph ML         | Методы обработки и агрегации данных в графовых структурах                               | GNN для предсказания связи fraud detection между пользователем и транзакцией.                              |
| Efficient DL     | Оптимизация моделей (квантизация, отсечение, дистилляция) для быстрого инференса         | DistilBERT — облегчённая версия BERT с минимальным падением качества.                                       |
| Generative DL    | Разработка генеративных моделей (VAE, GAN, Diffusion) для условной и мультимодальной генерации | Stable Diffusion позволяет создавать артконтент по текстовым описаниям.                                      |

---

## Общие термины, инструменты и ссылочки

### Common Operations
| Термин             | Определение                                                                            | Пример/Комментарий                                                                                          |
|--------------------|----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| StepLR             | Поэтапное понижение LR каждые K шагов обучения                                         | После каждых 10 000 batch уменьшаем lr на 0.1.                                                              |

### Common Tasks
| Термин             | Определение                                                                            | Пример/Комментарий                                                                                          |
|--------------------|----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| Distillation       | Преподавание "мягких" ответов большой модели компактной                                | Teacher = BERT large, student = DistilBERT. Тут student учится повторять вероятности классов от teacher.         |
| Quantization       | Перевод весов и активаций в низкую битность (8,4,2 бита)                               | INT8-квантизация снижает память в 4 × при ~1% потере качества.                                             |
| Pretraining        | Обучение на большой общей задаче перед специфичным дообучением                         | Обучили Word2Vec на тексте всего Wikipedia, а затем дообучили на конкретном корпусе новостей.                |
| Metric Learning    | Обучение метрики или эмбеддингов для сравнения объектов                                | Siamese-сетка для face verification — расстояние в латенте между фото одного человека меньше порогового.      |

### Common Terms in Architectures
| Термин             | Определение                                                                            | Пример/Комментарий                                                                                          |
|--------------------|----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| Backbone           | Базовый "экстрактор" признаков сети, используемый в разных задачах                     | ResNet50 в качестве backbone на задаче сегментации изображений, к нему прилагается свой head.                |
| Head               | Конечная часть сети, отвечающая за конкретный вывод                                    | Classification head из пары FC-слоев над backbone в ResNet.                                                 |
| Gating Mechanism   | Механизм "врат", контролирующий поток информации внутри сети                           | В LSTM "forget gate" решает, какую часть старого состояния сохранить.                                        |

### Probability & Processes
| Термин              | Определение                                                                            | Пример/Комментарий                                                                                          |
|---------------------|----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| Markov Process      | Случайный процесс, где будущее состояние зависит только от текущего, а не от предшествующих |      |

### Интерпретируемость и регуляризация
| Термин             | Определение                                                                            | Пример/Комментарий                                                                                          |
|--------------------|----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| Data Noising       | Добавление шума к входным данным для повышения устойчивости и обобщения                | Добавили к изображениям случайные пятна искажения, чтобы сетка не училась на "идеальных" данных.                |

### Libraries & Tools
| Термин              | Определение                                                                            | Пример/Комментарий                                                                                          |
|---------------------|----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| PyTorch             | Самый удобный фреймворк для создания нейронок и обучения                          |                                              |
| albumentations      | Либа для аугментаций изображений                                              | Легко добавить случайный кроп, поворот, изменение HSV для разнообразия данных.                               |
| einops              | Интуитивные операции по перестановке и преобразованию тензоров                         | `rearrange(x, 'b (h p1) (w p2) c -> b h w (p1 p2 c)')` — легко менять форму батча картинок.                 |
| Hugging Face        | "Система" для работы с трансформерами (модели, токенизаторы, датасеты)        | Загружаем pre-trained BERT, дообучаем и запускаем pipeline для анализа тональности в несколько строк кода (можно еще вывестив  "прод" свою приложуху").  |
| Optuna              | Фреймворк для автоматического поиска оптимальных гиперпараметров                       | Оптимизируем lr, batch size и количество слоёв через TPE-алгоритм, параллельно на нескольких GPU.            |
| Streamlit           | Инструмент для быстрого развёртывания ML-приложений в виде веб-интерфейсов              | За 10 минут превратили Jupyter-ноут в дашборд с графиками и слайдерами. |



### Libraries & Tools
| Термин              | Определение                                                                            | Пример/Комментарий                                                                                          |
|---------------------|----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| PyTorch             | Самый удобный фреймворк для создания нейронок и обучения                          |                                              |
| albumentations      | Либа для аугментаций изображений                                              | Легко добавить случайный кроп, поворот, изменение HSV для разнообразия данных.                               |
| einops              | Интуитивные операции по перестановке и преобразованию тензоров                         | `rearrange(x, 'b (h p1) (w p2) c -> b h w (p1 p2 c)')` — легко менять форму батча картинок.                 |
| Hugging Face        | "Система" для работы с трансформерами (модели, токенизаторы, датасеты)        | Загружаем pre-trained BERT, дообучаем и запускаем pipeline для анализа тональности в несколько строк кода (можно еще вывестив  "прод" свою приложуху").  |
| Optuna              | Фреймворк для автоматического поиска оптимальных гиперпараметров                       | Оптимизируем lr, batch size и количество слоёв через TPE-алгоритм, параллельно на нескольких GPU.            |
| Streamlit           | Инструмент для быстрого развёртывания ML-приложений в виде веб-интерфейсов              | За 10 минут превратили Jupyter-ноут в дашборд с графиками и слайдерами. |

### Полезные материалы
| | Ссылки | Описание | 
|---------------------|----------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| 1.|  [MLOps от ODS](https://www.youtube.com/@ПавелКикин-с6щ)  | Курс от Паши Кикина, от А до Я про деплой и разработку | 
| 2.|  [Хендбук от Yandex](https://education.yandex.ru/handbook/ml)  | Хорошая база по ML | 
| 3.|  [Лекции по матану от Шапошникова](https://teach-in.ru/course/calculus-shaposhnikov-part1)  | |
| 4.|  [Лекции по матану от Карасёва](https://rkarasev.ru/note/50)  | Есть задачник|
| 5.|  [Лекции по диффурам](https://www.youtube.com/live/tzvCXtPWGss)  | | 
| 6.|  [GANSхристианандерсон](https://education.yandex.ru/handbook/ml/article/generativno-sostyazatelnye-seti-(gan))  | Хендбук от яндекса|
| 7.|  [RL](https://education.yandex.ru/handbook/ml/article/obuchenie-s-podkrepleniem)  | Хендбук от яндекса|
| 8.|  [Временные ряды](https://education.yandex.ru/handbook/ml/article/vremennye-ryady)  | Хедбук от яндекса|
| 9.|  [Статья на хабре про TS](https://habr.com/ru/companies/skillfactory/articles/860660/)  | Хорошие ссылки на ресурсы + основы|
| 10.|  [Всё про флоу](https://akosiorek.github.io/norm_flows/)  | |
| 11.|  [RNN](https://deepmachinelearning.ru/docs/Neural-networks/Recurrent-neural-nets/RNN)  | |
| 12.|  [CNN](https://www.v7labs.com/blog/convolutional-neural-networks-guide)  | |
| 13.|  [Про трансформеры](https://education.yandex.ru/handbook/ml/article/transformery)  | Хеднбук от яндекса|
| 14.|  [Граф NN](https://education.yandex.ru/handbook/ml/article/grafovye-nejronnye-seti)  | Хендбук от яндекса |


### Улучшение курса
1. Тотально не хватает лекции по MLOps, хотя бы поверхностая лекции по виртуальным окружениям, hydra, docker, CI/CD, мониторингу данныъ/моделей и т.п.. В общем, всё про продакшен разработку
2. Было бы славно увидеть про оптимизацию в DL, про их инференс, ONNX.
3. Мечта - одна из лабораторных работ, которая выполнялась бы в команде. Что-то по типу финального проекта, где в конце студента представляют работающий сервис, рассказывают коротко про реализацию и внесённый вклад каждым участником команды.
4. Добавить гость-лекцию про system design.